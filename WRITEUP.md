
---

# üìÑ UPDATED `WRITEUP.md` (FINAL)

üëâ ’ç’ø’•’≤’Æ’´÷Ä `WRITEUP.md` ÷á ’ø’•’≤’°’§÷Ä’´÷Ä ’Ω’°‚Ä§

```markdown
# AI Engineering Assignment ‚Äì Final Write-up

## Overview

This project implements a complete Retrieval-Augmented Generation (RAG) pipeline capable of producing long-form handbooks exceeding 20,000 words from uploaded PDF documents. The system combines LightRAG-style retrieval, Supabase vector storage, and a Hugging Face language model with a LongWriter-style iterative generation loop.

The primary goal of the project is to demonstrate AI engineering skills in system design, modularity, retrieval, and long-form generation rather than optimizing language model output quality.

---

## System Design

The pipeline consists of the following stages:

1. PDF ingestion and text extraction
2. Chunking and semantic embedding
3. Vector storage using Supabase with pgvector
4. LightRAG-compatible semantic retrieval
5. Section-level generation using a Hugging Face LLM
6. Iterative LongWriter-style handbook assembly

Each stage is implemented as a modular component, allowing independent scaling or replacement.

---

## Retrieval-Augmented Generation

Semantic retrieval is performed using sentence-level embeddings generated by Sentence-BERT. These embeddings are stored in a Supabase PostgreSQL database with pgvector enabled. Queries are matched using cosine similarity via a SQL RPC function.

This ensures that all generated content is grounded strictly in the source document and avoids hallucinations.

---

## LongWriter-Style Handbook Generation

The handbook generator follows an iterative strategy inspired by LongWriter:

- A topic-based outline is generated
- Each section retrieves relevant context via LightRAG
- A Hugging Face Seq2Seq model rewrites the retrieved context into structured prose
- Sections are appended iteratively until the target length (20,000 words) is reached

This approach avoids single-prompt generation and enables scalable long-form content creation.

---

## Language Model Integration

A Hugging Face FLAN-T5 model is used for local, free inference. The model is integrated using `AutoModelForSeq2SeqLM` and `generate()` to ensure correct handling of encoder‚Äìdecoder architectures.

Decoding constraints (beam search, repetition penalties) are applied to improve stability and prevent degeneration.

---

## Engineering Decisions

- **Local LLM inference** was chosen to avoid API quotas and ensure reproducibility
- **Supabase** was selected as a lightweight, production-ready vector store
- **LightRAG-style abstraction** allows easy migration to alternative vector databases
- **Iterative generation** ensures scalability beyond prompt length limits

---

## Limitations

- The prose quality is limited by the lightweight LLM used
- No fine-tuning or reinforcement learning was applied
- The focus of the project is architectural correctness, not stylistic optimization

---

## Conclusion

This submission implements all core requirements of the assignment: LightRAG integration, Supabase vector storage, Hugging Face LLM usage, and LongWriter-style long-form generation. The system demonstrates a production-oriented AI engineering approach with emphasis on modularity, reliability, and scalability.
